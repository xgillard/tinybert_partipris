{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94f3661-6c05-40fd-9ab3-f957299ec129",
   "metadata": {},
   "source": [
    "# Configuration Globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9bf3a3-ad5f-447e-9269-c87eeb26cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import math\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn   as nn\n",
    "\n",
    "from tqdm         import tqdm\n",
    "from datasets     import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertConfig, BertForPreTraining\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "# import evaluate (if you intend to use metrics during training -- but it OOMS on manneback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fafa87-4eea-4db2-a225-32531df171e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_ROUND  = False  # is this the very first time that we want to start training this model ?\n",
    "SOURCE       = \"xaviergillard/parti-pris-v2-f32\"\n",
    "TARGET       = \"xaviergillard/parti-pris-v2-f32\"\n",
    "DTYPE        = torch.float32\n",
    "BATCH_SZ     =     8 # 64 on manneback\n",
    "TRAIN_EPOCHS =     3 # 92 over the course of 20h on manneback. I performed 3 rounds \n",
    "MAX_LENGTH   =  1024\n",
    "VOCAB_SIZE   = 30522\n",
    "HIDDEN_SIZE  =   384\n",
    "NUM_HIDDEN_L =     6\n",
    "NUM_ATTN_HEAD=     6\n",
    "INTERMEDIATE =  3072"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d3667-d91f-4f68-ac6d-6dd186982e7d",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf7a7a3-b0f0-4304-98bf-b847d06efbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIRST_ROUND:\n",
    "    ds = Dataset.from_csv(\"corpus_partipris_v2.csv\")\n",
    "    ds = ds.remove_columns([\"Unnamed: 0\", \"index\"])\n",
    "    ds = ds.map(lambda r: {'all_texts': r['title']+'\\n'+r['full_text']})\n",
    "    ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd3c5e-1ea5-4ce1-bae3-ce77d7ee04b5",
   "metadata": {},
   "source": [
    "# Creation du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864d4096-8d6c-4161-9eae-65cfac831aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer\n",
    "if FIRST_ROUND:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(SOURCE).train_new_from_iterator(iter(ds['all_texts']), vocab_size=VOCAB_SIZE)\n",
    "    tokenizer.push_to_hub(TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ed244-4e67-48c9-aa1f-660bea74248e",
   "metadata": {},
   "source": [
    "# Creation du dataset avec les NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4358a6db-1814-4df5-82a1-87a01a1b7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: encode the texts with the tokenizer using a smaller window than that of the model. \n",
    "#         this way we will be able to fit 2 chunks next to one another and still fit within\n",
    "#         the trained model context window.\n",
    "if FIRST_ROUND:\n",
    "    tokenize_half_size = {\n",
    "        'truncation': True,\n",
    "        'max_length': (MAX_LENGTH-3)//2,\n",
    "        'return_overflowing_tokens': True,\n",
    "        'stride': 2\n",
    "    }\n",
    "    \n",
    "    ds   = ds.map(lambda row: {'encoded': tokenizer.encode(row['all_texts'], **tokenize_half_size)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9ef5d9-6045-4a54-89cc-d766ce14647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: perform a pass over the encoded texts and create 1 datapoint for each chunk of data.\n",
    "#         for each of these, we flip a coin (proba 1/2) to decide whether or not the 2nd half\n",
    "#         of the context window will comprise the next sentence.\n",
    "if FIRST_ROUND:\n",
    "    data = ds['encoded']\n",
    "    N    = len(data)\n",
    "    info = []\n",
    "    for i,text in tqdm(enumerate(data)):\n",
    "        for j,chunk in enumerate(text):\n",
    "            nsp_label = j < len(chunk) - 1 and bool(random.getrandbits(1))\n",
    "            other= \"\"\n",
    "            if nsp_label: \n",
    "                other = chunk[j+1]\n",
    "            else:\n",
    "                next  = int(math.floor(random.random() * N))\n",
    "                other = data[next][0]\n",
    "            that = tokenizer.decode(other)\n",
    "            a = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "            b = tokenizer.decode(other, skip_special_tokens=True)\n",
    "            record = tokenizer(a, b, max_length=MAX_LENGTH, truncation=False, padding=False)\n",
    "            record['next_sentence_label'] = 0 if nsp_label else 1\n",
    "            info.append(record)\n",
    "    \n",
    "    data = pd.DataFrame(info)\n",
    "    data.to_parquet('partipris_pretraining_full.parquet')\n",
    "    data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d362df5-1615-40ab-b3c8-94e7d1a2ba20",
   "metadata": {},
   "source": [
    "# Entrainement du Modele a proprement parler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e012407a-78ab-4644-bcfa-2d6f734b58d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xavie\\.conda\\envs\\transformers-cuda\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='5760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  38/5760 00:17 < 47:24, 2.01 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 103\u001b[0m\n\u001b[0;32m     71\u001b[0m args      \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     72\u001b[0m     num_train_epochs            \u001b[38;5;241m=\u001b[39m TRAIN_EPOCHS,\n\u001b[0;32m     73\u001b[0m     per_device_train_batch_size \u001b[38;5;241m=\u001b[39m BATCH_SZ,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     hub_strategy                \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevery_save\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     91\u001b[0m     hub_token                   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_kaRjKTEWSbDJGYKcDdDoWUpAXlQQjtqJmE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     94\u001b[0m     model           \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m     95\u001b[0m     tokenizer       \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m#compute_metrics = compute_metrics\u001b[39;00m\n\u001b[0;32m    101\u001b[0m ) \n\u001b[1;32m--> 103\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# the end\u001b[39;00m\n\u001b[0;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(TARGET)\n",
      "File \u001b[1;32m~\\.conda\\envs\\transformers-cuda\\Lib\\site-packages\\transformers\\trainer.py:2043\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 2043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2044\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2045\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2046\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2047\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2048\u001b[0m     )\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2050\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32m~\\.conda\\envs\\transformers-cuda\\Lib\\site-packages\\transformers\\trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "if FIRST_ROUND:\n",
    "    data      = data\n",
    "    tokenizer = tokenizer\n",
    "    config    = BertConfig(\n",
    "        vocab_size              = VOCAB_SIZE,\n",
    "        hidden_size             = HIDDEN_SIZE,\n",
    "        num_hidden_layers       = NUM_HIDDEN_L,\n",
    "        num_attention_heads     = NUM_ATTN_HEAD,\n",
    "        intermediate_size       = INTERMEDIATE,\n",
    "        max_position_embeddings = MAX_LENGTH)\n",
    "    model     = BertForPreTraining(config)\n",
    "else:    \n",
    "    data      = Dataset.from_pandas(pd.read_parquet('partipris_pretraining_full.parquet'))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TARGET)\n",
    "    model     = BertForPreTraining.from_pretrained(SOURCE, torch_dtype=DTYPE) \n",
    "\n",
    "# split test train\n",
    "data = data.train_test_split(train_size=0.95, shuffle=True)\n",
    "\n",
    "#########################################################################################\n",
    "# metrics\n",
    "#########################################################################################\n",
    "# metrics are disabled because unfortunately, it causes cuda OOM on manneback\n",
    "#########################################################################################\n",
    "#loss     = nn.CrossEntropyLoss()\n",
    "#accuracy = evaluate.load(\"accuracy\")\n",
    "#def ignoring_dummy(preds, labels, dummy=-100):\n",
    "#    yhat = []\n",
    "#    y    = []\n",
    "#    labels = labels.reshape((-1,))\n",
    "#    preds  = preds.reshape((labels.shape[0], -1))    \n",
    "#    for i,label in enumerate(labels):\n",
    "#        if label == dummy:\n",
    "#            continue\n",
    "#        else:\n",
    "#            y.append(label)\n",
    "#            yhat.append(preds[i].argmax())\n",
    "#    yhat = np.array(yhat)\n",
    "#    y    = np.array(y)\n",
    "#    return (yhat, y)\n",
    "#    \n",
    "#def compute_metrics(eval):\n",
    "#    y_mlm, y_nsp = eval.label_ids\n",
    "#    h_mlm, h_nsp = eval.predictions\n",
    "#    #\n",
    "#    y_mlm = torch.tensor(y_mlm.reshape((-1,))).to('cpu')\n",
    "#    h_mlm = torch.tensor(h_mlm.reshape((y_mlm.shape[0], -1))).to('cpu')\n",
    "#    l_mlm = loss(h_mlm, y_mlm)\n",
    "#    \n",
    "#    y_nsp = torch.tensor(y_nsp.reshape((-1,))).to('cpu')\n",
    "#    h_nsp = torch.tensor(h_nsp.reshape((y_nsp.shape[0], -1))).to('cpu')\n",
    "#    l_nsp = loss(h_nsp, y_nsp)\n",
    "#    #\n",
    "#    h_mlm, y_mlm = ignoring_dummy(h_mlm, y_mlm, dummy=-100)\n",
    "#    a_mlm = accuracy.compute(predictions=h_mlm, references=y_mlm)\n",
    "#    a_nsp = accuracy.compute(predictions=h_nsp.argmax(axis=-1), references=y_nsp)\n",
    "#    #\n",
    "#    return {\n",
    "#        'mlm_accuracy': a_mlm['accuracy'], \n",
    "#        'nsp_accuracy': a_nsp['accuracy'], \n",
    "#        'mlm_loss': l_mlm, \n",
    "#        'nsp_loss': l_nsp, \n",
    "#        'tot_loss': l_mlm + l_nsp \n",
    "#    }\n",
    "#\n",
    "#########################################################################################\n",
    "\n",
    "# training\n",
    "collator  = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
    "args      = TrainingArguments(\n",
    "    num_train_epochs            = TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size = BATCH_SZ,\n",
    "    #\n",
    "    output_dir                  = './checkpoints', \n",
    "    overwrite_output_dir        = True,\n",
    "    save_strategy               = \"epoch\", \n",
    "    save_total_limit            = 2,\n",
    "    #\n",
    "    eval_strategy               = \"epoch\",\n",
    "    #eval_steps                  = 1,\n",
    "    #eval_accumulation_steps     = 1,\n",
    "    #torch_empty_cache_steps     = 1,\n",
    "    #\n",
    "    gradient_accumulation_steps = 2, # batch de 64 etait ok\n",
    "    bf16                        = (DTYPE == torch.bfloat16),\n",
    "    #\n",
    "    push_to_hub                 = True,\n",
    "    hub_model_id                = TARGET,\n",
    "    hub_strategy                = \"every_save\",\n",
    "    hub_token                   = \"USE_YOUR_OWN\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    tokenizer       = tokenizer,\n",
    "    train_dataset   = data['train'], \n",
    "    eval_dataset    = data['test'],\n",
    "    args            = args,\n",
    "    data_collator   = collator,\n",
    "    #compute_metrics = compute_metrics\n",
    ") \n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# the end\n",
    "model.push_to_hub(TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf425d-077f-4054-8b51-c6ccfa6b7be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
